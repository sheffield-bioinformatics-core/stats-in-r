---
title: "Statistical Analysis of Biological Data"
author: "Mark Dunning and Aya Elwazir"
date: '`r format(Sys.time(), "Last modified: %d %b %Y")`'
output: 
  html_notebook: 
    toc: yes
    toc_float: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE)
```

# Acknowledgement

Some of these materials are based on previous courses at Cancer Research Uk Cambridge Institute.

- [Introduction to Statistical Analysis](http://bioinformatics-core-shared-training.github.io/IntroductionToStats/)
- [Further Statistical Analysis in R](https://rawgit.com/bioinformatics-core-shared-training/intermediate-stats/master/manual.pdf)

# Part I - Contingency tables

When working with categorical variables, we are usually interested in the frequencies of the different categories in our sample. To display data for two or more categorical variables, cross-tabulations, or contingency tables, are commonly used - with 2 x 2 tables being the simplest. We can then test whether there is an association between the row factor and the column factor by a *chi-squared test* or a *Fisher’s exact test*.

To demonstrate the analysis of contingency tables we will use a dataset provided with the `vcd` package. You will need to install this package using the R command

**Note** although the `tidyverse` collection of packages is much more convenient for many analyses, there are functions in *"base R"* that provide a more convenient interface for analysis of count data.

```{r eval=FALSE}
install.packages("vcd")
```


The data frame `Arthritis` should then be accessible which is described as:- 

> Data from Koch & Edwards (1988) from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis.

```{r}
#let's use the'Arthritis' dataset in the 'vcd' package 
library(vcd)
head(Arthritis)
```

In base R, we can extract a column from a table using the `$` notation. With tab-completion the column can be selected without introducing and typos. The `table` function can then be used directly to give tabulation.

```{r message=FALSE}

##one way table (count of one variable)
t1 <- table(Arthritis$Sex)
t1
```

We can quickly visualise these counts as a *barplot*. A pie chart is possible, but not generally recommended.

```{r}
barplot(t1)
```


Further manipulation of this table will give proportions (which will ultimately be used in the statistical testing)

```{r}
#to get proportion of males and females

prop.table(t1)
prop.table(t1)*100  #to make it a percentage
```

The `table` function can also compare two columns from a data frame if both columns are given as arguments.

```{r}
##two way table (cross tab)
t2 <- table (Arthritis$Sex, Arthritis$Improved) #swap variables to switch row and column position
t2
```

These can also be plotted 

```{r}
plot(t2)
barplot(t2)
barplot(t2, beside=TRUE)
```

The `prop.table` function can also be applied to these tables. By default the calculated proportions will be a proportion of the total number of observations.

```{r}
#proportion & percentage
prop.table(t2)          #proportion from the total
prop.table(t2)*100      #percentage from the total
```

An extra argument can also be supplied to allow the proportions to be expressed as a proportion of the first or second variable.

```{r}
prop.table(t2,1)       #proportion from the first variable
prop.table(t2,1)*100    #percentage from the first variable
```

```{r}
prop.table(t2,2)        #proportion from the second variable
prop.table(t2,2)*100    #percentage from the second variable

```

We can add these sums to our table using the `addmargins` function.

```{r}
#sum
addmargins(t2)          #both sums shown 
addmargins(t2,1)        #sum of first variable
addmargins(t2,2)        #sum of second variable

```

A three-way table is also possible

```{r}
##three-way contingency table
t3 <- table (Arthritis$Sex, Arthritis$Improved, Arthritis$Treatment)
t3

```

A slightly more convenient output can be obtained using the `ftable` function

```{r}
#we can also put them all in a multi-dimensional table
t4 <- ftable(Arthritis$Sex, Arthritis$Improved, Arthritis$Treatment)
t4
```

Having explored our data, we can now perform statistical testing. The function `chisq.test` can be used to assess whether differences in proportions are significant or not. We actually don't need to calculate the proportions; R will do this for us.


```{r}
chiTable <- table(Arthritis$Improved, Arthritis$Treatment)
chisq.test(chiTable)

```

However, the `chisq.test` function is not appropriate in all circumstances.

```{r}
t2
chisq.test(t2) #notice the warning message

```


```{r}
#Fisher exact
fisher.test(t2)

```

```{r}
#consider adding arguments for: 
    #alternative (two sided[default] or one sided)
    #confid. level (0.95 [default], 0.99 or any) eg:
fisher.test(t2, alternative = "one-sided", conf.level = 0.99)

```

Although not covered in the lectures, the `mantelhaen.test` function can be used to assess significance for a three-way table.

```{r}
##3 variables
t3
mantelhaen.test(t3)
```



<div class="alert alert-warning">

**Exercise**

1- Read the excel file called `Ex Biostat P1.xlsx` into R (see below for the required code)

2- Make a cross table showing the gender in the rows and tumor grade in the columns

3- Define the percentage of Grade III tumors within females 

4- Add a column in the table showing the sum of the 3 grades in males and in females and state the total number of males and females in the sample 

5- Use the appropriate test to check if the tumor grade depends on the gender 

```{r eval=FALSE}
tab <- readxl::read_xlsx("data/EX Biostat P1.xlsx")
```

</div>




# Part II - How to assess normality

We will read some example data to illustrate how one would test for a normally-distributed variable. This property is important as it influences which test we should use.

One of the best ways of displaying data is by using a graph. Graphs can make both simple and complex data easier to understand by making it easier to spot trends and patterns. We can use plots to view the distribution of our data (minimum, maximum, mid-point, spread etc) and to ensure that the values in our dataset seem realistic (e.g. no outliers). Many statistical tests rely on the assumption that the data are normally distributed.

The data for this section are to be found in the file `normal_example.csv` in the `data` folder. You will need to specify the file path accordingly.

```{r message=FALSE}
library(readr)
df1 <- read_csv("data/normal_example.csv")
```

We can inspect the data in RStudio and discover that it consists of a single column called `x` that comprises numeric observations

```{r}
View(df1)
```

Various graphical methods are available to assess the distrbution. The first of which is a *histogram*. Here, the data are split into "bins" (`ggplot2` choses the number of bins) and the value on the `y` axis corresponds to the number of observations in that bin. The user only has to specify the variable to be plotted, and `ggplot2` takes care of the binning. From this plot we can judge what the average value of the data is and the spread.

```{r}
library(ggplot2)
ggplot(df1, aes(x=x)) + geom_histogram()
```

A similar option is to produce a density curve. Here the y-axis is the *density* of a particular value.

```{r}
ggplot(df1, aes(x = x)) + geom_density()
```

### Combining Histograms and Density 

When assessing the distribution of a variable, you might be tempted to plot the histogram and density on the same plot. Conventional `ggplot` would suggest the following code

```{r}
ggplot(df1, aes(x=x)) + geom_histogram() + geom_density()
```

However, this doesn't work. If you check the y-axis limits on the histogram and density plots, you'll notice they are on a different scale. After some digging around the solution is [as follows](http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/)


```{r}
ggplot(df1, aes(x=x)) + geom_histogram(aes(y=..density..)) + geom_density()
```

Although we generally recommend `ggplot2`, the "base" `hist` function sometimes does a better job of choosing the bin widths for the histogram. It is worth looking at the base plot if the `ggplot` doesn't look particularly convincing.

```{r}
hist(df1$x,freq =FALSE)
lines(density(df1$x),col="blue")
```



A box plot is an excellent way of displaying continuous data when you are interested in the spread of your data. The box of the box plot corresponds to the lower and upper quartiles of the respective observations and the bar within the box, the median. The whiskers of the box plot correspond to the distance betweenthe lower/upper quartile and the smaller of: the smaller/largest measurement *OR* 1.5 times the inter quartile range. A disadvantage of the box plot is that you don’t see the exact data points. However, box plots are very useful in large datasets where plotting all of the data may give an unclear picture of the shape of your data.

```{r}
ggplot(df1, aes(x="",y=x)) + geom_boxplot()
```

A *violin plot* is sometimes used in conjunction with the boxplot to show density information.

```{r}
ggplot(df1, aes(x="",y=x))  + geom_violin() + geom_boxplot()
```

Individual points can also be added with `geom_jitter`; avoiding over-plotting by adding random noise along the x-axis.

```{r}
ggplot(df1, aes(x="",y=x))  + geom_violin() + geom_boxplot() + geom_jitter(width=0.1)
```


Finally, we have a "*qq-plot*" which allows to compare the quantiles of our dataset against a theoretical normal distribution. If the majority of points lie on a diagonal line then the data are approximately normal.

```{r}
ggplot(df1,aes(sample=x)) + geom_qq() + geom_qq_line(col="red")
```

These graphical methods are by far the easiest way to assess if a given dataset is normally-distributed.

For "real-life" data, the results are unlikely to give a perfect plot, so some degree of judgement and prior experience with the data type are required.  Indeed, it should be noted that the dataset visualised in the above plots was sampled from a normal distribution. Even then, the plots were not 100% convincing!

## Tests for normality

Although their usage is contentious amongst statisticians, there are a few methods for testing whether variables are normally-distributed or not. If the p-value is sufficiently small then we conclude that the data are not normally distributed. However, some statisticians prefer to use graphical methods and their intuition about the data or prior knowledge of the data type (e.g. some measures are generally believed to be normally-distributed)

```{r}

#shapiro test
#p<0.05 ...difference between data and normality..data not normal
#p>0.05 ...no diff between data and normality ..data normally distributed

shapiro.test(df1$x)

```

```{r}
#kolomogorov-Smirnov test
ks.test(df1$x,"pnorm", mean=mean(df1$x), sd=sd(df1$x))

#what test to choose
#kolomogorov-Smirnov test...sample >=1000
#Shapiro....smaller samples but more strict(<5000)
```

## Descriptive Statistics

In the [accompanying R course](http://sbc.shef.ac.uk/r-crash-course/) we have seen how to produce summary statistics of columns in a dataset. For a dataset that is normally-distributed, appropriate measures of the average and variability are the *mean* and *standard deviation*. Both these functions are available within R and can be used in conjunction with the `summarise` function in `dplyr`.

```{r}
library(dplyr)
summarise(df1, Mean = mean(x), 
          Var= var(x),
          SD = sd(x))
```

<div class="alert alert-warning">
**Exercise**

1- Read the excel file called `Ex Biostat P2.xlsx` into R 

2- Identify if the age and hospitalization days are normally distributed

3- Use the appropriate descriptive statistics [mean and SD,  or median and IQR] for each variable

</div>



# Part III - Significance tests for continuous variables

In this part we will show how to perform tests to compare 1, 2 (or more) continuous variables. The dataset, provided by MASH at The University of Sheffield, describes individuals that have been following different diets and their age and gender. The main goal of interest is to determine which of three competing diet regimes results in the greatest weight loss.

```{r message=FALSE}
diet <- read_csv("data/diet.csv")
diet
```



## One-sample test

The first hypothesis we will test is whether the people in the study are overweight or not. This first involves some manipulation of the table to calculate an extra variable; the Body Mass Index (BMI). We will test if people in our study are overweight, where overweight is defined as having a BMI over 25.

$BMI = weight  / height^2$

*where the weight is measured in kg and the height in metres*

<div class="alert alert-warning">
**Exercise**

- Add a new variable to the data frame for the BMI of each person 
    + you might want to do this in multiple steps using the `%>%` notation
</div>

```{r echo=FALSE}
diet <- mutate(diet,BMI = initial.weight / (height/100)^2)

```

We can now test our new variable for normality using the plots and tests from earlier

```{r eval=FALSE}
ggplot(diet, aes(x = BMI)) + geom_histogram()

```

The one-sample t-test is implemented in the function `t.test`. The only input we need to specify is a *vector* of numeric values and it will perform a statistical test. For simplicity, we can use the `$` notation in R to extract the column. This will provide us with a *t* test statistic and a p-value. **However, it is up to us to interpret the p-value**.

```{r}
t.test(diet$BMI)
```

We get a significant result! However, if we look at the description for `t.test` it is testing against a population mean of $0$. No wonder that we get a significant result. By changing the `mu` argument we perform the desired test.

```{r}
t.test(diet$BMI, mu=25,alternative = "greater")
```


##  Two-sample tests

A two-sample t-test should be used if you want to compare the measurements of two populations. There are two types of two-sample t-test:independent (unpaired) and paired (dependent). To make the correct choice, you need to understand your underlying data. 

An independent two-sample t-test is used when the two samples are independent of each other, e.g. *comparing the mean response of two groups of patients on treatment vs. control in a clinical trial*. 

As the name suggests,a paired two-sample t-test is used when the two samples are paired, e.g. *comparing the mean blood pressure of patients before and after treatment* (two measurements per patient).

We might wonder if there is actually any effect due to diet. To perform this test we can compare two columns from the `diet` data frame directly using the `$` notation.

```{r}
t.test(diet$final.weight, diet$initial.weight)
```

The p-value is significant and shows that overall the weights of individuals is *different* before and after diet. However, this test is not specifically testing for a *decrease* after the diet (which we would really hope to be the case). By adding an extra argument `alternative=less` we get a different result

```{r}
t.test(diet$final.weight, diet$initial.weight,alternative = "less")
```


There is also extra information that we could employ; namely that the measurements of weight are made **on the same person** before and after dieting. This is a classic example of when to apply a paired t-test. We do not need to change the code much to perform such a test; only add an argument `paired=TRUE` to `t.test`.

```{r}
t.test(diet$final.weight, diet$initial.weight,alternative = "less", paired=TRUE)
```

The output of the test is quite informative as it suggest that testing has been performed on sets of differences. In actual fact, this is the same as performing  a *one-sample test* on the differences we have already calculated:-

```{r}
diet <- mutate(diet, weight.loss = initial.weight - final.weight)
t.test(diet$weight.loss)
```

## The Independant t test.......TWO independant groups 

Lets consider that we want to compare whether males or females lost more weight during the trial. Here we have two groups and these can be treated as *independant* variables as different patients belong to the two groups.

The *null hypothesis* for such a test would be that the weight loss is the same between groups male and female. We seek to evidence to reject this hypothesis by calculating a test statistic. Firstly, we have to check for normality:-

```{r}
ggplot(diet, aes(x = gender, y = weight.loss)) + geom_boxplot()

group_by(diet, gender) %>% 
  summarise(shapiro=shapiro.test(weight.loss)$p.value,variance=var(weight.loss))
```

We conclude that the variances are approximately the same and both variables are normally-distributed


We can now use the `t.test` function to perform an *independant* test. The first argument to `t.test` is the *R formula* notation for the test being performed. It allows us to define which columns in our dataset are the numeric and grouping variables in the test.

This function allows various type of test to be performed by changing the appropriate arguments (see the help for `t.test` for details (`?t.test`)). For instance, we can tell the test that we believe our variances are equal or not.

```{r}
t.test(weight.loss ~ gender, data=diet, var.equal=TRUE)
```

We can see that the t-statistic we observe is consistent with the null hypothesis, that the weight loss in males and females is the same. That is, the probability of observing a t-statistic of 0.2 or more, or -0.2 or less, is quite high.

*This is not a significant result (p>0.05), so there is no evidence of a difference in weight loss between males and females*

## Non- Parametric alternatives (Wilcoxon test)

Being able to use the `t-test` relies on the your data being normally-distributed. If we do not sufficient confidence in this assumption, there are different statistical tests that can be applied. Rather than calculating and comparing the *means* and *variances* of different groups they are *rank-based* methods. However, they still come with a set of assumptions and involve the generation of test statistics and p-values.

### Independant samples = Wilcoxon rank sum test (Mann Whitney U test)

This test has many different names including the Wilcoxon, Wilcoxon two sample test, Wilcoxon-Mann-Whitney, Wilcoxon rank sum and the Mann-Whitney-U test. However, this test should not be confused with the Wilcoxon signed rank test (which is used for paired tests). To avoid confusion this test is usually referred to as the Mann-Whitney U test, which is used when the dependent variable to be examined is continuous but the assumptions for parametric tests are violated.

The assumptions of the Mann-Whitney U are as follows:

1.The dependent variable is ordinal or continuous.

2.The data consist of a randomly selected sample of independent observations from two independent groups.

3.The dependent variables for the two independent groups share a similar shape.

Fortunately, the R programmers have made the function to do a wilcox test similar to doing a t-test. **The difficulty is in choosing the correct test to apply - which R will not advise you on**.

Let's go back to our example of comparing weight loss between groups male and female. The equivalent non-parametric version of the test we performed before is:-

```{r}
wilcox.test(weight.loss~gender, data=diet)
```

The `wilcox.test` is flexible in much the same way that `t.test` in. We can switch to applying a paired test by adding the argument `paired=TRUE`.

```{r}
wilcox.test(diet$final.weight,diet$initial.weight, paired=TRUE)
```

There is no difference in weight loss between males and females. However, the test of interest is whether some diets are more beneficial. If we attempt to test the difference between diets, the following happens:-

```{r eval=FALSE}
t.test(weight.loss ~ diet.type,data=diet)
```

In other words, the t-test is only applicable for comparing two groups and we have three diets that we want to compare.

## Compare between *more than two* groups

#### Parametric (ANOVA)

The two-sample t-test is useful when we have just two groups of continuous data to compare. When we want to compare more than two groups, a one-way ANOVA can be used to simultaneously compare all groups, rather than carrying out several individual two-sample t-tests.  The main advantage of doing this is that it reduces the number of tests being carried out, meaning that the type I error rate (the probability of seeing a significant result just by chance) does not become inflated. 

In order to justify if an ANOVA test is appropriate we have to test for normality.

```{r}
ggplot(diet, aes(x = diet.type, y = weight.loss)) + geom_violin()
group_by(diet, diet.type) %>% 
  summarise(shapiro=shapiro.test(weight.loss)$p.value,variance=var(weight.loss))
```

A one-way ANOVA compares group means by partitioning the variation in the data into *between group variance* and *within group variance*. Like the other statistical tests we have encountered, the functions in R do the hard work of calculating the statistics. Performing the analysis is in two stages; fit the model using the `aov` function and then assess the significance using the `summary` function

```{r}
anova <- aov(weight.loss ~ diet.type, data=diet)
summary(anova)
```

When the test provides a significant result it tells us that there is at least on difference in the groups. However, it does not tell us which group is different. For this, we can apply a "post-hoc test" such as the Tukey test. If `summary(anova)` did not produce a significant p-value, we wouldn't proceed with this step


```{r}
TukeyHSD(anova)
```

```{r}
plot(TukeyHSD(anova))
```


### Non-Parametric (Kruskal Wallis) 

Data that do not meet the assumptions of ANOVA (e.g. normality) can be tested using a non-parametric alternative. The *Kruskal-Wallis* test is derived from the one-way ANOVA, but uses ranks rather than actual observations. It is also the extension of the Mann-Whitney U test to greater than two groups. 

```{r}
kruskal.test(weight.loss ~ as.factor(diet.type), data=diet)
```

Like the one-way ANOVA this will only tell us that at least one group is different and not specifically which group(s). The post-hoc `dunn.test` is recommended which also performs multiple testing correction.

```{r}
library(dunn.test)
dunn.test(diet$weight.loss,diet$diet.type,method = "Bonferroni")
```

At this point we could be about to recommend diet C to those that wish to lose weight. However, are there any other factors in the data that we should be considering? With `ggplot2` we can quite easily visualise the effects of multiple factors on the data. Lets add both gender and diet type into the plot. It now appears that diet C is having an effect on males but not females.

```{r}
ggplot(diet, aes(x = diet.type, y = weight.loss,fill=gender)) + geom_boxplot()
```

## Two-way ANOVA

The *formula* notation allows us to specify an *interaction* between gender and diet type. In other words, we are looking to see if the effect of diet type is different for males and females.

```{r}
anova <- aov(weight.loss ~ diet.type*gender, data=diet)
summary(anova)
```

This tells us that an effect exists between diet type and gender, but like before we have to run a post-hoc test to discover more

```{r}
TukeyHSD(anova)
```

```{r}
TukeyHSD(anova)

```

## Repeated Measures

We will read a modified version of the diet dataset in order to test a "repeated measures" analysis. Here we have added a midpoint weight measurement.

```{r}
diet2 <- read_csv("data/diet2.csv")
View(diet2)
```

However, the three measures that we want to compare are given as columns in the data frame. We cannot express this using the R `~` notation. In other words the data is in *wide* format and not *long*. We can change this using the `reshape2` package. This creates a variable for the type of measurement (initial / mid / final) and a value.

```{r}
diet_melt <- diet2 %>% select(contains("weight")) %>% 
  reshape2::melt()
```

```{r}
summary(aov(value~variable,data=diet_melt))
```




<div class="alert alert-warning">
**Exercise**

The excel file ‘RCC2’ contains data about the expression levels of some genes in patients with renal cell carcinoma. In your study, you put the following hypotheses.
Please test those alternative hypotheses and state whether you will accept or reject each one.

1.	Females have a higher level of E2F3 than males 
2.	ANXA expression levels vary between unilateral and bilateral tumors
3.	Individuals with RCC grade II have different levels of E2F3 than those with grade III or IV 
4.	The mean value of miR499 decreases significantly after treatment
5.	DFFA is higher in patients with grade IV tumors
</div>

# Solutions

**To be revealed during the workshop!**

